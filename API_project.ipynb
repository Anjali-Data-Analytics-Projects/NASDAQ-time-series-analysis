{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e4bd27-ba80-450a-951f-78234f1162bd",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3206cbce-d45b-4da1-9b37-a3bc6100a0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf9c2c-54ee-4263-ac75-6501ca6ac115",
   "metadata": {},
   "source": [
    "#### Fetch Data from Nasdaq API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afdd4f5-848f-4e32-9fae-59c428830545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To determine the appropriate API endpoint and the parameters needed to query the financial data, refer to the Nasdaq Data Link API documentation for guidance on constructing your query.         \n",
    "\n",
    "# Function to fetch data from the API\n",
    "def fetch_data(url, API_key, cursor=None, limit=1000):\n",
    "    params = {\"qopts.per_page\": limit,\n",
    "             \"api_key\" : API_key\n",
    "             }\n",
    "    if cursor:\n",
    "        params['qopts.cursor_id'] = cursor\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81609ec-b7d6-4b69-a25d-3385833643ec",
   "metadata": {},
   "source": [
    "#### Data Integration into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ef4f96-cb7e-4d1d-baa5-d6f54128c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the API\n",
    "base_url = \"https://data.nasdaq.com/api/v3/datatables/MER/F1.json\"\n",
    "limit = 1000  # Number of items per page\n",
    "API_Key = \"9LVNHxksgJPUXGKbCDXw\" #The Nasdaq Data Link API requires an API key for access.\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Initialize cursor to start the first page\n",
    "cursor = None\n",
    "\n",
    "while True:\n",
    "    # Fetch data from the API\n",
    "    response_data = fetch_data(base_url, API_Key, cursor, limit)\n",
    "    \n",
    "    # Extract the datatable, columns, and data\n",
    "    datatable = response_data.get('datatable', {})\n",
    "    data = datatable.get('data', [])\n",
    "    columns_info = datatable.get('columns', [])\n",
    "    \n",
    "    # Extract column names\n",
    "    column_names = [col['name'] for col in columns_info]\n",
    "    \n",
    "    # Convert data to a DataFrame\n",
    "    if data:\n",
    "        page_df = pd.DataFrame(data, columns = column_names)\n",
    "        \n",
    "        # Append the DataFrame for the current page to the combined DataFrame\n",
    "        combined_df = pd.concat([combined_df, page_df], ignore_index=True)\n",
    "    \n",
    "    # Extract the next cursor, if it exists\n",
    "    next_cursor = response_data.get('meta', {}).get('next_cursor_id')\n",
    "    \n",
    "    # Break the loop if there are no more pages\n",
    "    if not next_cursor:\n",
    "        break\n",
    "    \n",
    "    # Update cursor for the next page\n",
    "    cursor = next_cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adacab36-1552-4dd9-afad-5d8c840f3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "combined_df.to_csv(\"raw_financial_data.csv\", index=False)\n",
    "# When you're working with data in memory (e.g., within a Python session or a Jupyter notebook), that data will be lost once the session ends or the notebook is closed. \n",
    "#Saving the DataFrame to a CSV file ensures that the data is stored persistently on disk and can be accessed later.\n",
    "\n",
    "# Print the DataFrame to confirm\n",
    "print(\"Data saved to raw_financial_data.csv\")\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"raw_financial_data.csv\")\n",
    "# Print the number of rows and columns\n",
    "num_rows, num_columns = df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26861911-fe61-4cb6-8d5f-552eb6d3a1b1",
   "metadata": {},
   "source": [
    "#### Api Exchange Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bbb0094-db89-4baa-a75d-7d8aa6476b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your API key and base URL for ExchangeRate API\n",
    "api_key = 'd851ddbf6ea7708ef2df52fc'\n",
    "base_currency = 'USD'\n",
    "base_url = f'https://v6.exchangerate-api.com/v6/{api_key}/latest/{base_currency}'\n",
    "\n",
    "# Fetch exchange rates from API\n",
    "response = requests.get(base_url)\n",
    "data = response.json()\n",
    "\n",
    "# Check if request was successful\n",
    "if data['result'] == 'success':\n",
    "    exchange_rates = data['conversion_rates']  # Dictionary of currency exchange rates\n",
    "else:\n",
    "    print(\"Error fetching exchange rates:\", data['error-type'])\n",
    "    exchange_rates = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a841e5a-e44b-4fac-927f-833c40d0741e",
   "metadata": {},
   "source": [
    "####  Removing leading/trailing spaces and Standardized DataFrame Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309312a-72f3-4ca7-b144-172c033ebd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing leading/trailing spaces and Standardized DataFrame Column Names \n",
    "df.columns = df.columns.str.strip().str.title()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d6d4b-9afd-49b9-b6a4-b23727c41055",
   "metadata": {},
   "source": [
    "#### Removed Duplicate Entries Rows to Ensure Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e773b-a8e5-4b30-8b3b-0bd9e32ef8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True) # Removed Duplicate Entries Rows to Ensure Data Quality\n",
    "# This method identifies and removes any duplicate rows in the DataFrame. By default, it checks all columns.\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6178bf1b-cd60-4139-bdd6-bb9d071df72b",
   "metadata": {},
   "source": [
    "#### Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "001310a6-a47e-4a20-a206-47bcd1428f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Forward filling the 'Auditorstatus' column\n",
    "df['Auditorstatus'] = df['Auditorstatus'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7683959c-f5b9-4be6-8280-a894f12204c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute the mode for each country\n",
    "mode_df = df.groupby('Country')['City'].agg(lambda x: x.mode()[0] if not x.mode().empty else None).reset_index()#  Groups the data by the 'Country' column and focuses on the 'City' column. Applies a lambda function to each group. The lambda function calculates the mode (most frequent value) of cities. If the mode is not empty, it takes the first mode value; otherwise, it returns None.Converts the grouped result back into a DataFrame.\n",
    "#print(mode_df)\n",
    "mode_df.columns = ['Country', 'CityMode']# Renames the columns for clarity, where 'CityMode' represents the most frequent city for each country.\n",
    "#print(mode_df)\n",
    "# Step 2: Merge the mode_df with the original DataFrame to fill missing cities\n",
    "df = df.merge(mode_df, on='Country', how='left')\n",
    "\n",
    "# Fill missing values in 'City' with the mode\n",
    "df['City'] = df['City'].fillna(df['CityMode'])\n",
    "\n",
    "# Drop unnecessary 'CityMode' column\n",
    "df.drop(columns=['CityMode'], inplace=True)\n",
    "\n",
    "# Step 3: Drop rows where 'City' is still missing\n",
    "df = df.dropna(subset=['City'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babf1ade-9116-4734-b1be-00873c595a0d",
   "metadata": {},
   "source": [
    "#### Data Transformation\n",
    "Convert columns into the appropriate data types (e.g., datetime for time, floats for numerical values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1001c9d5-4b6e-43d2-8cf7-6e0549b2a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types\n",
    "\n",
    "# Optimized Memory Usage by Converting Categorical Data\n",
    "# Convert categorical columns to category type\n",
    "categorical_columns = ['Reporttype', 'Auditorstatus', 'Currency', 'Consolidated', 'Status', \n",
    "                      'Country', 'Indicator', 'Statement']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2f877-61c0-4dc6-ad97-68b971b1710b",
   "metadata": {},
   "source": [
    "#### Streamlining Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0857cc22-7097-4673-929a-46b041415603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlined Dataset by Dropping Irrelevant Columns (to improve data relevance and focus on key variables for analysis.)\n",
    "# Remove irrelevant or duplicate information\n",
    "df.drop(columns=['Address1', 'Address2', 'Address3', 'Address4', 'Phonenumber', 'Faxnumber','Website', 'Statecode', 'Fye', 'Region', 'Cik', 'Mic', 'Ticker', 'Zipcode', 'Countrycode', 'Mapcode', 'Compnumber', 'Shortname' ], inplace=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd711f2c-5bcf-44e1-b0f7-1d676c167b7b",
   "metadata": {},
   "source": [
    "#### Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fc08940-b936-4dcb-a482-40ea901c8eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['Longname', 'Reportdate'], ascending=[True, False])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d4942-0ab8-4a0e-9c51-e85546792c0d",
   "metadata": {},
   "source": [
    "#### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86bf6e9d-b6ac-4ada-9f16-160543d32306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapped Country Codes to Full Names for Enhanced Readability\n",
    "# Create a dictionary for mapping country codes to country names\n",
    "country_dict = {\n",
    "    'USA': 'United States',\n",
    "    'GBR': 'United Kingdom',\n",
    "    'CYM': 'Cayman Islands',\n",
    "    'CAN': 'Canada',\n",
    "    'KOR': 'South Korea',\n",
    "    'ISR': 'Israel',\n",
    "    'FIN': 'Finland',\n",
    "    'JPN': 'Japan',\n",
    "    'IND': 'India',\n",
    "    'DNK': 'Denmark',\n",
    "    'IRL': 'Ireland',\n",
    "    'CHL': 'Chile',\n",
    "    'ITA': 'Italy',\n",
    "    'ESP': 'Spain',\n",
    "    'BHS': 'Bahamas',\n",
    "    'HKG': 'Hong Kong',\n",
    "    'FRA': 'France',\n",
    "    'DEU': 'Germany',\n",
    "    'BEL': 'Belgium',\n",
    "    'BMU': 'Bermuda',\n",
    "    'CHE': 'Switzerland',\n",
    "    'IDN': 'Indonesia',\n",
    "    'AUS': 'Australia',\n",
    "    'BRA': 'Brazil'\n",
    "}\n",
    "# Assuming 'df' is your DataFrame and 'country_code' is the column with country codes\n",
    "df['Country'] = df['Country'].map(country_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ab43d-7ee7-4230-be73-90c9866735ec",
   "metadata": {},
   "source": [
    "#### Handle Time-Series Inconsistencies\n",
    "Ensure the time column is complete without missing dates or timestamps, and in the right format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bdcc87a-682e-4140-9ec8-d442511fa2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'reportdate' to datetime\n",
    "df['Reportdate'] = pd.to_datetime(df['Reportdate'], errors='coerce') # Standardized Date Formats for Time Series Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e59ff-5ebe-432a-8034-aae97e626e66",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9dc6c05-bdcf-413c-afed-d98ce96d6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all amounts to USD based on the fetched exchange rates\n",
    "df['Amount_USD'] = df.apply(lambda row: row['Amount'] * exchange_rates.get(row['Currency'], 1), axis=1)\n",
    "#The lambda function is applied to each row of the DataFrame df. exchange_rates.get(row['Currency'], 1) retrieves the exchange rate for the currency in the current row. If the currency is not found, it defaults to 1 (no conversion). If the currency is not found, it defaults to 1 (no conversion).applies this function to each row (axis=1), creating a new column 'Amount_USD' with the converted values.\n",
    "# Drop the original currency column if needed\n",
    "df = df.drop(columns=['Currency','Amount'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62da5eff-a663-45c1-b745-f64ebb596022",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524501b-f2b4-4832-af18-cc586704a277",
   "metadata": {},
   "source": [
    "+ **Descriptive Statistics:** Compute summary statistics for numeric columns.\n",
    "+ **Distribution Analysis:** Plot the distribution of key variables (e.g., financial amounts).\n",
    "+ **Time Series Analysis:** Analyze and plot how financial metrics change over time.\n",
    "+ **Correlation Analysis:** Examine relationships between numerical columns.\n",
    "+ **Categorical Analysis:** Assess the distribution of categorical variables (e.g., indicators, statuses)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571523dd-59ef-4c9f-a86a-7976d9d5cebf",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1c759-1554-4238-a9ab-8bec0a61b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any missing values in 'amount' column\n",
    "amount_data = df['Amount_USD'].dropna()\n",
    "\n",
    "# Descriptive statistics\n",
    "summary_stats = amount_data.describe()\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(summary_stats)\n",
    "\n",
    "# Additional statistics\n",
    "median_amount = amount_data.median()\n",
    "variance_amount = amount_data.var()\n",
    "print(f\"Median of 'amount': {median_amount}\")\n",
    "print(f\"Variance of 'amount': {variance_amount}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3aa282-9b61-45b6-928c-e1f6c026cc49",
   "metadata": {},
   "source": [
    "### Categorical Variables Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93a5ed-7a1d-442b-bce6-ce055bab0e70",
   "metadata": {},
   "source": [
    "#### Pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97550e5b-3fca-4a7b-885d-a0178a0f9cb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories = ['Reporttype', 'Auditorstatus', 'Country', 'Statement']\n",
    "\n",
    "# Plot pie charts for categorical variables\n",
    "for col in categories:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    df[col].value_counts().plot.pie(autopct='%1.1f%%', colors=sns.color_palette('viridis', len(df[col].unique())))\n",
    "    plt.title(f'Proportion of Categories in {col}')\n",
    "    plt.ylabel('')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bef6d-043a-462b-acfc-c7d20734f7b1",
   "metadata": {},
   "source": [
    "#### Barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284e8be-a096-4b45-a92b-aad9a5469afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a color palette with 53 distinct colors\n",
    "palette = sns.color_palette(\"husl\", 53)\n",
    "\n",
    "# Create a dictionary mapping each indicator to a color\n",
    "indicator_list = df['Indicator'].value_counts().index\n",
    "color_map = dict(zip(indicator_list, palette))\n",
    "\n",
    "# Plot the count plot with custom colors\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.countplot(data=df, x='Indicator', order=indicator_list, palette=color_map)\n",
    "plt.xticks(rotation=90)  # Rotate labels for better readability\n",
    "plt.title(\"Distribution of Financial Indicators with Custom Colors\")\n",
    "plt.xlabel(\"Indicator\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcdfb1a-6337-4e45-932f-ca7a0bb0092f",
   "metadata": {},
   "source": [
    "## Time-Series-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9695ac1-4990-4cfa-a521-46f03d97eda1",
   "metadata": {},
   "source": [
    "###  Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf08acb-1c60-43cd-aa29-6bb70e697ad0",
   "metadata": {},
   "source": [
    "#### Load your time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a36ceb2-1694-4ee8-a490-ad145393603a",
   "metadata": {},
   "source": [
    "##### Aggregate Data by Time Period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2ba76-8df6-4e3d-a314-68f89c4106b5",
   "metadata": {},
   "source": [
    "##### Aggregate financial amounts by different time periods (e.g., monthly, quarterly, yearly) to observe trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e19c63-c983-45ed-96fd-1e4026ef726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Developed Interactive Data Aggregation Tool\n",
    "\n",
    "# Get all unique company names (longname) and display them\n",
    "unique_companies = df['Longname'].unique()\n",
    "print(\"Unique Companies: \")\n",
    "for idx, name in enumerate(unique_companies, start=1):\n",
    "    print(f\"{idx}. {name}\")\n",
    "\n",
    "# Prompt the user to select a company by entering the corresponding number\n",
    "company_index = int(input(f\"\\nEnter the number corresponding to the company you want to analyze (1-{len(unique_companies)}): \"))\n",
    "\n",
    "# Get the selected company name based on the user's input\n",
    "company_name = unique_companies[company_index - 1]\n",
    "\n",
    "# Filter the DataFrame for the selected company name and create a copy\n",
    "company_data = df[df['Longname'] == company_name].copy()\n",
    "\n",
    "# Extract 'Reportdate' and 'Amount' columns\n",
    "company_data = company_data[['Reportdate', 'Amount_USD']]\n",
    "\n",
    "# Ensure 'reportdate' is set as the index and convert to datetime if necessary\n",
    "company_data.loc[:, 'Reportdate'] = pd.to_datetime(company_data['Reportdate'])\n",
    "\n",
    "# Optionally, you might want to sort the data by 'Reportdate'\n",
    "company_data = company_data.sort_values(by='Reportdate')\n",
    "\n",
    "# Reset the index to remove the old index\n",
    "company_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a4a7f2-f65b-42b7-b9b1-3aa8304ed571",
   "metadata": {},
   "source": [
    "#### Distribution Analysis(Numeric Variable(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d08028-326f-41d3-b31c-a28d349aa831",
   "metadata": {},
   "source": [
    "+ Histogram with KDE:\n",
    "    +  The sns.histplot() plots a histogram for the numeric variable along with the KDE (Kernel Density Estimation), which gives a smoothed line of the distribution.\n",
    "    + Boxplot: The sns.boxplot() shows the distribution of data, highlighting the median, quartiles, and outliers for each numeric variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23293cd6-99bd-47f6-9af6-09831785b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Histogram with KDE for numeric variable\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(company_data['Amount_USD'], kde=True, bins=30, color='blue')\n",
    "plt.title('Histogram and KDE of Amount column values Distribution')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Boxplot for numeric variable\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=company_data['Amount_USD'], color='green')\n",
    "plt.title('Boxplot of Amount column values Distribution')\n",
    "plt.xlabel('Amount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c014d6c5-c3ba-4409-a72d-8ecf5ed6a394",
   "metadata": {},
   "source": [
    "#### Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69357935-cf0d-47c4-95b7-7c545901e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of values in the 'Amount' column of the DataFrame df.\n",
    "# Box Plot to visualize outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=company_data['Amount_USD'])\n",
    "plt.title(\"The distribution of values in the 'Amount' column\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram for the 'amount' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(company_data['Amount_USD'], bins=30, edgecolor='black')\n",
    "plt.title('Frequency distribution of Amount variable')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Violin plot for 'amount' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x=company_data['Amount_USD'])\n",
    "plt.title('Violin Plot for Amount')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e9272e-9eec-408c-b02a-12b76cc74aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate IQR\n",
    "# Identified Outliers Using IQR Method\n",
    "amount_data = company_data['Amount_USD']\n",
    "Q1 = amount_data.quantile(0.25)\n",
    "Q3 = amount_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outlier boundaries\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = amount_data[(amount_data < lower_bound) | (amount_data > upper_bound)]\n",
    "print(f\"Number of outliers: {len(outliers)}\") # Print Number of Outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80030aad-80b3-42f7-acaf-8c24bc0286e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "company_data = company_data[(company_data['Amount_USD'] >= lower_bound) & (company_data['Amount_USD'] <= upper_bound)]\n",
    "\n",
    "# Plot histogram of data without outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['Amount_USD'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Amounts (Without Outliers)')\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac524b-54e4-4fe8-bfa2-aa870b4830cd",
   "metadata": {},
   "source": [
    "#### Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b70e1a4-2599-4490-b474-e5d4f2d2bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the month, quarter, and year from the 'Reportdate' column\n",
    "company_data['Month'] = company_data['Reportdate'].dt.month\n",
    "company_data['Quarter'] = company_data['Reportdate'].dt.quarter\n",
    "company_data['Year'] = company_data['Reportdate'].dt.year\n",
    "\n",
    "company_data.set_index('Reportdate', inplace=True)#Setting the index for time-based resampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b0a684-a498-42fc-bca8-bd6c56279b9e",
   "metadata": {},
   "source": [
    "#### Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acc2fd02-225b-4976-81f7-8a2cddca811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregating_financial_metrics():\n",
    "    global monthly_amounts, quarterly_amounts, annual_amounts\n",
    "    # Aggregate by month\n",
    "    monthly_amounts = company_data['Amount_USD'].resample('ME').sum()  # 'ME' stands for Month-End frequency\n",
    "    \n",
    "    # Aggregate by quarter\n",
    "    quarterly_amounts = company_data['Amount_USD'].resample('QE').sum()  # 'QE' stands for Quarter-End frequency\n",
    "    \n",
    "    # Aggregate by year\n",
    "    annual_amounts = company_data['Amount_USD'].resample('YE').sum()  # 'YE' stands for Year-End frequency\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13bc9091-a8b6-4d43-b9f1-f765402caa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function with the user-selected company name\n",
    "aggregating_financial_metrics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3a059-6d57-462f-81e0-1d7230a34b84",
   "metadata": {},
   "source": [
    "####  Lag Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cbbd41d-87f6-471e-bcac-c9bd04341398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by 'Reportdate'\n",
    "company_data = company_data.sort_values(by='Reportdate')\n",
    "\n",
    "# Create lagged features for 'Amount' column\n",
    "company_data['Lag_1'] = company_data['Amount_USD'].shift(1)  # Previous period's Amount (1 period lag)\n",
    "company_data['Lag_2'] = company_data['Amount_USD'].shift(2)  # Two periods ago\n",
    "company_data['Lag_3'] = company_data['Amount_USD'].shift(3)  # Three periods ago\n",
    "\n",
    "# Drop rows where lagged values are NaN\n",
    "company_data = company_data.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ffdd5-0091-484b-9a74-4829287617f5",
   "metadata": {},
   "source": [
    "#### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75692d77-649e-46d3-8486-57b39c01c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between the original 'Amount' and lagged features\n",
    "correlation_matrix = company_data[['Amount_USD', 'Lag_1', 'Lag_2', 'Lag_3']].corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix between Amount and Lagged Features:\")\n",
    "print(correlation_matrix)\n",
    "# Define target column and lag columns\n",
    "target_column = 'Amount_USD'\n",
    "lag_columns = ['Lag_1', 'Lag_2', 'Lag_3']\n",
    "\n",
    "# Interpretation of the correlation matrix\n",
    "print(\"\\nInterpretation:\")\n",
    "print()\n",
    "    \n",
    "# Diagonal elements (self-correlation) are always 1.0\n",
    "print('Diagonal Elements (Self-Correlation)')\n",
    "print(f\"The diagonal elements represent the self-correlation, which is always 1.0.\")\n",
    "print()\n",
    "\n",
    "lag_period = {\n",
    "    'Lag_1' : 'from the previous period',\n",
    "    'Lag_2' : 'from two periods ago',\n",
    "    'Lag_3' : 'from three periods ago'\n",
    "}\n",
    "\n",
    "# Interpret the correlation between the target column and lagged features\n",
    "print(\"Correlation Between Amount and Lagged Features:\")\n",
    "for lag_column in lag_columns:\n",
    "    correlation_value = correlation_matrix.loc[target_column, lag_column]\n",
    "    print(f\"The correlation between the {target_column} and {lag_column} is {correlation_value:.4f}.\", end=\" \")\n",
    "    if abs(correlation_value) > 0.7:\n",
    "        print(f\"- Strong correlation detected.\")\n",
    "        print(f\"This suggests a strong relationship, meaning that past amounts {lag_period[lag_column]} are quite similar to the current amount.\")\n",
    "        print()\n",
    "    elif abs(correlation_value) > 0.3:\n",
    "        print(f\"- Moderate correlation detected.\")\n",
    "        print(f\"This indicates a moderate relationship, suggesting some influence of past amounts {lag_period[lag_column]} on the current amount.\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"- Weak correlation detected.\")\n",
    "        print(f\"This shows a weak relationship, implying that past amounts {lag_period[lag_column]} do not have much impact on the current amount.\")\n",
    "        print()\n",
    "        \n",
    "\n",
    "# Interpret the correlation between lagged features\n",
    "print('Correlation Between Lagged Features')\n",
    "for i, col1 in enumerate(lag_columns):\n",
    "    for col2 in lag_columns[i+1:]:\n",
    "        correlation_value = correlation_matrix.loc[col1, col2]\n",
    "        print(f\"The correlation between {col1} and {col2} is {correlation_value:.4f}.\", end=\" \")\n",
    "        if abs(correlation_value) > 0.7:\n",
    "            print(f\"- Strong correlation detected.\")\n",
    "            print(f\"This indicates that past amounts from the {col1} and {col2} periods are quite similar.\")\n",
    "            print()\n",
    "        elif abs(correlation_value) > 0.3:\n",
    "            print(f\"- Moderate correlation detected.\")\n",
    "            print(f\"This shows a moderate similarity between past amounts from the {col1} and {col2} periods.\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"- Weak correlation detected.\")\n",
    "            print(f\"This suggests that the past amounts from the {col1} and {col2} periods are not very similar.\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9754a2-5e2f-4fc6-adce-ebf8a117f664",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0519c-96d9-4317-a236-70d3007b2608",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the figure size for the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot heatmap using seaborn\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "\n",
    "# Set the title\n",
    "plt.title('Correlation Heatmap of Financial Metrics')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2a65d-e602-4c0e-a810-05036b6593fe",
   "metadata": {},
   "source": [
    "#### Visualize Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567f8aae-6a95-4ec3-adf1-2d403a67cd58",
   "metadata": {},
   "source": [
    "Create plots to visualize the aggregated data to identify trends, seasonality, and anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c022a4e-d547-4885-a4cb-764ce69c08ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_financial_data(financial_data, period):\n",
    "    # Set plot properties based on the period\n",
    "    if period == 'Month':\n",
    "        color = 'blue'\n",
    "        locator = mdates.MonthLocator()  # Ticks every month\n",
    "        formatter = mdates.DateFormatter('%b %Y')  # Month Year format\n",
    "    elif period == 'Quarter':\n",
    "        color = 'green'\n",
    "        locator = mdates.MonthLocator(interval=3)  # Ticks every 3 months (quarterly)\n",
    "        formatter = mdates.DateFormatter('%b %Y')  # Month Year format\n",
    "    elif period == 'Year':\n",
    "        color = 'red'\n",
    "        locator = mdates.YearLocator()  # Ticks every year\n",
    "        formatter = mdates.DateFormatter('%Y')  # Year format\n",
    "    else:\n",
    "        raise ValueError(\"Invalid period. Choose from 'Month', 'Quarter', or 'Year'.\")\n",
    "\n",
    "    # Plot the financial data\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(financial_data.index, financial_data.values, marker='o', linestyle='-', color=color, label = f'{period}-Wise Trend')\n",
    "\n",
    "    # Format x-axis based on the time period\n",
    "    plt.gca().xaxis.set_major_locator(locator)\n",
    "    plt.gca().xaxis.set_major_formatter(formatter)\n",
    "\n",
    "    # Add title, labels, and legend\n",
    "    plt.title(f'{period}-Wise Financial Analysis')\n",
    "    plt.xlabel(f'{period}s')\n",
    "    plt.ylabel('Total Amount')\n",
    "    plt.grid(True)\n",
    "    #plt.legend(loc='best')\n",
    "    plt.legend(loc='upper left')\n",
    "    # Rotate x-axis labels by 90 degrees\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Adjust layout to prevent clipping of labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca1113-16d0-419d-8058-3e6ec0a827bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_financial_data(monthly_amounts, period='Month')\n",
    "plot_financial_data(quarterly_amounts, period='Quarter')\n",
    "plot_financial_data(annual_amounts, period='Year')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c668b9c9-f617-49fb-9428-2c4c16fbafcd",
   "metadata": {},
   "source": [
    "####  Outlier Identification Using Z-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01a2348d-82ed-47a7-8ade-777468f37ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Outlier Identification using Z-scores on the 'Amount_USD' column\n",
    "company_data['z_scores'] = stats.zscore(company_data['Amount_USD'])\n",
    "\n",
    "# Define the Z-score threshold for outlier detection\n",
    "threshold = 3\n",
    "\n",
    "# Identify rows where the absolute value of the Z-score is greater than the threshold\n",
    "outliers = company_data[np.abs(company_data['z_scores']) > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b61b2-0a29-4172-a023-7a35fac7bf6a",
   "metadata": {},
   "source": [
    "#### Visualization with Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70bbd7-345e-4398-a338-a9dabffe6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Visualization with Box Plot**\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x = company_data['Amount_USD'])\n",
    "plt.title('Box Plot of Amount')\n",
    "plt.show()\n",
    "print('''\n",
    "The box represents the interquartile range (IQR), which contains 50% of the data points.\n",
    "The line inside the box represents the median.\n",
    "The \"whiskers\" extend to the data points within 1.5 times the IQR from the quartiles.\n",
    "Points outside the whiskers are considered potential outliers.\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfd7f4f-d423-4f3e-8f25-d996e84895ce",
   "metadata": {},
   "source": [
    "##### Visualization with Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cfa43b-ff46-40a6-bf0f-9ce821cffb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(company_data.index, company_data['Amount_USD'], label='Datapoints', color='blue')  # Fixed color\n",
    "plt.scatter(outliers.index, outliers['Amount_USD'], label='Outliers', color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Amount (USD)')\n",
    "plt.title('Scatter Plot with Outliers Highlighted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26b830-ca32-446f-b668-17ded21a8de1",
   "metadata": {},
   "source": [
    "##### Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be6798-eee5-46ea-807b-9d388e0630bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Impact Analysis: Sensitivity Analysis**\n",
    "# Remove outliers and compare trends\n",
    "company_data_cleaned = company_data.drop(outliers.index)  # Drop rows that are considered outliers\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot original data\n",
    "plt.plot(company_data.index, company_data['Amount_USD'], label='Original Data')\n",
    "\n",
    "# Plot cleaned data (without outliers)\n",
    "plt.plot(company_data_cleaned.index, company_data_cleaned['Amount_USD'], label='Cleaned Data', linestyle='--')\n",
    "\n",
    "# Labeling the axes\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Amount (USD)')\n",
    "plt.title('Impact of Outliers on Trend')\n",
    "\n",
    "# Show legend and plot\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6bb07-6f93-4dde-849a-eaa8407a37fa",
   "metadata": {},
   "source": [
    "#####  Decompose Time Series\n",
    "Decompose the time series into trend, seasonal, and residual components to better understand underlying patterns.      \n",
    "Decomposition: Separates the trend component from seasonal and residual components to clearly see the long-term movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76d16bc6-3058-437a-a66e-d79fc5312193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_decomposition(time_series_data, model='additive', period=None):\n",
    "    # Decompose the time series into observed, trend, seasonal, and residual components\n",
    "    decomposition = seasonal_decompose(time_series_data, model=model)\n",
    "    \n",
    "    # Plot the decomposed components\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(decomposition.observed, label='Observed', color='blue')\n",
    "    plt.title(f'Observed trend of {period}-Wise Series Analysis')\n",
    "    plt.xlabel(f'{period}s')\n",
    "    plt.ylabel('Total Amount')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(decomposition.trend, label='Trend', color='red')\n",
    "    plt.title(f'Trend Component of {period}-Wise Series Analysis')\n",
    "    plt.xlabel(f'{period}s')\n",
    "    plt.ylabel('Total Amount')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(decomposition.seasonal, label='Seasonal', color='green')\n",
    "    plt.title(f'Seasonal Component of {period}-Wise Series Analysis')\n",
    "    plt.xlabel(f'{period}s')\n",
    "    plt.ylabel('Total Amount')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(decomposition.resid, label='Residual', color='orange')\n",
    "    plt.title(f'Residual Component of {period}-Wise Series Analysis')\n",
    "    plt.xlabel(f'{period}s')\n",
    "    plt.ylabel('Total Amount')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "    # Make sure the layout is neat\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_time_series_decomposition(monthly_amounts, model='additive', period=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349f24b-1a3c-4521-8efa-d64db2628199",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series_decomposition(monthly_amounts, model='additive', period= 'Month')\n",
    "plot_time_series_decomposition(quarterly_amounts, model='additive', period= 'Quarter')\n",
    "plot_time_series_decomposition(annual_amounts, model='additive', period= 'Year')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d983c9-3776-451e-9e81-f16bb24ab3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Augmented Dickey-Fuller test on the 'Amount_USD' column\n",
    "result = adfuller(company_data['Amount_USD'])\n",
    "\n",
    "# Extract and display relevant test results\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "print(f'Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'   {key}: {value}')\n",
    "\n",
    "# Detailed interpretation of the result\n",
    "if result[1] < 0.05:\n",
    "    print(\"\\nInterpretation: The p-value is less than 0.05, indicating that the series is stationary.\")\n",
    "    print(\"This means we reject the null hypothesis (which states that the series has a unit root and is non-stationary).\")\n",
    "    print(\"Therefore, the 'Amount_USD' series can be considered to have constant mean and variance over time, which makes it suitable for many forecasting models.\")\n",
    "else:\n",
    "    print(\"\\nInterpretation: The p-value is greater than 0.05, indicating that the series is non-stationary.\")\n",
    "    print(\"This means we fail to reject the null hypothesis, suggesting that the 'Amount_USD' series has a unit root and is non-stationary.\")\n",
    "    print(\"The series likely exhibits trends or seasonality, which can cause the mean and variance to change over time. In this case, differencing or transformations may be required to achieve stationarity.\")\n",
    "\n",
    "# Interpretation of ADF statistic and critical values\n",
    "if result[0] < result[4]['1%']:\n",
    "    print(\"\\nThe ADF statistic is smaller than the 1% critical value, indicating strong evidence of stationarity at the 1% significance level.\")\n",
    "elif result[0] < result[4]['5%']:\n",
    "    print(\"\\nThe ADF statistic is smaller than the 5% critical value, indicating evidence of stationarity at the 5% significance level.\")\n",
    "elif result[0] < result[4]['10%']:\n",
    "    print(\"\\nThe ADF statistic is smaller than the 10% critical value, indicating weak evidence of stationarity at the 10% significance level.\")\n",
    "else:\n",
    "    print(\"\\nThe ADF statistic is larger than the 10% critical value, indicating that the series is likely non-stationary.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab0bf3-0215-4ebf-82db-cb5ecb6b029d",
   "metadata": {},
   "source": [
    "##### ROLLING STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "403048e4-2116-4f1e-9e81-ef97003daec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize global variables\n",
    "rolling_mean = None\n",
    "rolling_std = None\n",
    "rolling_window = None\n",
    "Time_period = None\n",
    "def plot_rolling_statistics(amounts, rolling_window_param, time_period):\n",
    "    global rolling_mean, rolling_std, rolling_window, Time_period\n",
    "    rolling_window = rolling_window_param  # Set the global variable\n",
    "    Time_period = time_period\n",
    "    # Calculate rolling mean and standard deviation\n",
    "    rolling_mean = amounts.rolling(window=rolling_window).mean()\n",
    "    rolling_std = amounts.rolling(window=rolling_window).std()\n",
    "\n",
    "    # Plot the time series with rolling statistics\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Original time series plot\n",
    "    plt.plot(amounts.index, amounts.values, \n",
    "             label=f'Original {Time_period}-Wise Time Series', \n",
    "             color='blue', marker='o', linestyle='-')\n",
    "    \n",
    "    # Rolling mean and standard deviation plots\n",
    "    plt.plot(rolling_mean.index, rolling_mean.values, \n",
    "             label=f'{rolling_window}-{Time_period} Rolling Mean', \n",
    "             color='red')\n",
    "    \n",
    "    plt.plot(rolling_std.index, rolling_std.values, \n",
    "             label=f'{rolling_window}-{Time_period} Rolling Std', \n",
    "             color='green')\n",
    "    \n",
    "    # Titles and labels\n",
    "    plt.title(f'Time Series with {rolling_window}-{Time_period} Rolling Mean & Std of {Time_period}-Wise Amounts')\n",
    "    plt.xlabel(f'{Time_period}')\n",
    "    plt.ylabel('Amount')\n",
    "    \n",
    "    # Legend and grid\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Interpretation\n",
    "    print(f\"\\nInterpretation of Rolling Statistics:\")\n",
    "    print(f\"1. **Rolling Mean:** The rolling mean (shown in red) smooths out short-term fluctuations and highlights longer-term trends. \"\n",
    "          f\"This is particularly useful for identifying overall patterns in the data. If the rolling mean increases, \"\n",
    "          f\"it indicates a general upward trend in the monthly amounts over the specified window.\")\n",
    "    \n",
    "    print(f\"2. **Rolling Standard Deviation:** The rolling standard deviation (shown in green) measures the variability of the data points \"\n",
    "          f\"over the rolling window. A high standard deviation indicates greater variability in the data, while a low standard deviation suggests \"\n",
    "          f\"more consistency. Observing changes in the rolling standard deviation can signal shifts in the stability of the time series.\")\n",
    "    \n",
    "    print(f\"3. **Analysis of Trends:** Comparing the original time series with the rolling mean can help identify periods of change. \"\n",
    "          f\"If the original data points frequently deviate from the rolling mean, it may indicate periods of volatility or outliers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636af704-0a6a-4005-9dba-f59a696fe08d",
   "metadata": {},
   "source": [
    "##### Identify Anomalies\n",
    "Detect anomalies or outliers in the time series data which may indicate significant events or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc0b4343-62d1-4811-922b-acbc354281cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(Value, rolling_mean, rolling_std, rolling_window, threshold, time_period):\n",
    "    # Drop NaN values from rolling calculations\n",
    "    valid_indices = rolling_mean.dropna().index\n",
    "    rolling_mean = rolling_mean.dropna()\n",
    "    rolling_std = rolling_std.dropna()\n",
    "    Value = Value[valid_indices]\n",
    "    \n",
    "    # Detect anomalies using thresholds based on rolling mean and standard deviation\n",
    "    upper_threshold = rolling_mean + threshold * rolling_std\n",
    "    lower_threshold = rolling_mean - threshold * rolling_std\n",
    "    anomalies_upper = Value[Value > upper_threshold]\n",
    "    anomalies_lower = Value[Value < lower_threshold]\n",
    "\n",
    "    # Plot the time series with rolling statistics and anomalies\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(Value, label=f'{time_period} Aggregated Amount', color='blue', marker='o', linestyle='-')\n",
    "    plt.plot(rolling_mean, label=f'{rolling_window}-{time_period} Rolling Mean', color='red')\n",
    "    plt.plot(upper_threshold, label='Upper Threshold', color='green', linestyle='--')\n",
    "    plt.plot(lower_threshold, label='Lower Threshold', color='green', linestyle='--')\n",
    "    \n",
    "    # Plot the anomalies (both upper and lower)\n",
    "    plt.scatter(anomalies_upper.index, anomalies_upper, color='red', label='High Anomalies')\n",
    "    plt.scatter(anomalies_lower.index, anomalies_lower, color='orange', label='Low Anomalies')\n",
    "    \n",
    "    # Titles and labels\n",
    "    plt.title(f'{time_period} Aggregated Data with Anomalies')\n",
    "    plt.xlabel(f'{time_period}')\n",
    "    plt.ylabel('Amount')\n",
    "    \n",
    "    # Legend and grid\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48e0ce-35a4-4e93-a138-778cb6d24160",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling_statistics(monthly_amounts, 5 , 'Month')\n",
    "detect_anomalies(monthly_amounts, rolling_mean, rolling_std, rolling_window, 2, Time_period)\n",
    "plot_rolling_statistics(quarterly_amounts, 3 , 'Quarter')\n",
    "detect_anomalies(quarterly_amounts, rolling_mean, rolling_std, rolling_window, 2, Time_period)\n",
    "plot_rolling_statistics(annual_amounts, 2 , 'Year')\n",
    "detect_anomalies(annual_amounts, rolling_mean, rolling_std, rolling_window, 2, Time_period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d493b-5450-41ed-b160-f2a8f7137f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
